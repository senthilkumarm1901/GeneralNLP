{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import warnings\n",
    "from fuzzywuzzy import fuzz\n",
    "import numpy as np\n",
    "import string\n",
    "import pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import dok_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizing=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "Regexp_tokenizing=RegexpTokenizer(\"\\w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip=pd.read_csv(r\"Anyfile.csv\",error_bad_lines=False,index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('CD'):\n",
    "        return wordnet.NOUN\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip['Lemmatized_Nouns']=''\n",
    "for each in range(len(ip)):\n",
    "\tif each%100==0: print each\n",
    "\ttext=ip.ix[each,'Content'].decode('utf8','ignore').encode('utf8','ignore')\n",
    "\ttry:\n",
    "\t\tsent_tokenize_list=nltk.sent_tokenize(text)\n",
    "\t\tRegEx_Tokenized_Words=[Regexp_tokenizing.tokenize(sentence) for sentence in sent_tokenize_list]\n",
    "\t\tPos_Tag_list=[nltk.pos_tag(word) for word in RegEx_Tokenized_Words]\n",
    "\t\tLemma_words_post=[]\n",
    "\t\tfor sentence in Pos_Tag_list:\n",
    "\t\t\tfor word, tag in sentence:\n",
    "\t\t\t\twntag = get_wordnet_pos(tag)\n",
    "\t\t\t\tif wntag!= None:\n",
    "\t\t\t\t\tLemma_words_post.append(lemmatizing.lemmatize(word,wntag))\n",
    "\t\t\t\t\ttext_lemma=' '.join(word for word in Lemma_words_post)\n",
    "\t\tip.ix[each,'Lemmatized_Nouns']=text_lemma\n",
    "\texcept:\n",
    "\t\ttemp=\"Ignoredocument\"\n",
    "\t\tip.ix[each,'Lemmatized_Nouns']=temp\n",
    "\t\tpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(word, blob):\n",
    "    return (float)(blob.words.count(word)) / (float)(len(blob.words))\n",
    "\n",
    "def n_containing(word, bloblist):\n",
    "    return (float)(sum(1 for blob in bloblist if word in blob))\n",
    "\n",
    "def idf(word, bloblist):\n",
    "    return (float)(math.log(len(bloblist)) / (float)(1 + n_containing(word, bloblist)))\n",
    "\n",
    "def tfidf(word, blob, bloblist):\n",
    "    return (float)((float)(tf(word, blob)) * (float)(idf(word, bloblist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF_Scores=[]\n",
    "bloblist= ip['Lemmatized_Nouns']\n",
    "for each in range(len(ip)):\n",
    "    if each%100==0: print each\n",
    "    blob=tb(ip.ix[each,'Lemmatized_Nouns'])\n",
    "    templist=list(blob.words)\n",
    "    tempdict= {word: tfidf(word, blob, bloblist) for word in templist}\n",
    "    tempdict_sorted= sorted(tempdict.items(), key=lambda x:x[1],reverse=True)\n",
    "    TFIDF_Scores.append(tempdict_sorted[0:14]) #only top 15 nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF_words=[None]*len(TFIDF_Scores)\n",
    "for each in range(len(TFIDF_Scores)):\n",
    "    TFIDF_words[each]=''\n",
    "    for every in TFIDF_Scores[each]:\n",
    "        TFIDF_words[each]=TFIDF_words[each]+' '+every[0]\n",
    "    TFIDF_words[each]=TFIDF_words[each].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip['TFIDF_Lemmatized_Nouns']=pd.Series(TFIDF_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip.to_csv(r'Lemmatized_TFIDF_Top15_noun_version.csv',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tag(tag):\n",
    "    \"\"\"Convert the tag given by nltk.pos_tag to the tag used by WordNetLemmatizer\"\"\"\n",
    "    \n",
    "    tag_dict = {'N': 'n', 'J': 'a', 'R': 'r', 'V': 'v'}\n",
    "    try:\n",
    "        return tag_dict[tag[0]]\n",
    "    except KeyError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lemma_list[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=ip.ix[0,'Content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenize_list=nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RegEx_Tokenized_Words=[Regexp_tokenizing.tokenize(sentence) for sentence in sent_tokenize_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pos_Tag_list=[nltk.pos_tag(word) for word in RegEx_Tokenized_Words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pos_Tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('RB'):\n",
    "        return wordnet.ADV\n",
    "    elif treebank_tag.startswith('CD'):\n",
    "        return wordnet.NOUN\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lemma_words_overall=[]\n",
    "\n",
    "text_lemma_list=[]\n",
    "for sentence in Pos_Tag_list:\n",
    "    for word, tag in sentence:\n",
    "        wntag = get_wordnet_pos(tag)\n",
    "        if wntag!= None:\n",
    "            Lemma_words_post.append(lemmatizing.lemmatize(word,wntag))\n",
    "            text_lemma=' '.join(word for word in Lemma_words_post)\n",
    "text_lemma_list.append(text_lemma)\n",
    "Lemma_words_overall.append(text_lemma_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lemma_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lemma_words_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip.ix[0,'Content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, tag in Pos_Tag_list[0]:\n",
    "    wntag = get_wordnet_pos(tag)\n",
    "    if wntag is None:# not supply tag in case of None\n",
    "        print word,tag, lemmatizing.lemmatize(word) \n",
    "    else:\n",
    "        print word,tag, lemmatizing.lemmatize(word, pos=wntag) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
