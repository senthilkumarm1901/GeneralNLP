{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Main Sources of Reference: \n",
    ">  - Evolution of TL in NLP: https://arxiv.org/pdf/1910.07370v1.pdf\n",
    ">  - ELMo Paper: https://arxiv.org/pdf/1802.05365.pdf <br>\n",
    ">    - ELMo in Articles:\n",
    ">     - https://ahmedhanibrahim.wordpress.com/2019/07/01/a-study-on-cove-context2vec-elmo-ulmfit-and-bert/\n",
    ">     - https://medium.com/@plusepsilon/the-bidirectional-language-model-1f3961d1fb27\n",
    ">  - ELMo in practice: https://colab.research.google.com/drive/13f6dKakC-0yO6_DxqSqo0Kl41KMHT8A1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolution of RNN architectures for Transfer Learning in NLP (Part 3)\n",
    "\n",
    "#### Already covered in Part 1\n",
    "- Introduction to Language Modeling\n",
    "- How Transfer Learning Evolved\n",
    "- Evolution of RNN units - RNN, LSTM, GRU, AWD-LSTM\n",
    "\n",
    "#### Agenda covered in Part 2\n",
    "- ULMFiT\n",
    "\n",
    "#### Agenda covered here in Part 3\n",
    "- ELMo\n",
    "_______________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ELMo comes up with better `word representations/embeddings` using Language Models that learn the `context` of the word in focus\n",
    "![](https://ahmedhanibrahim.files.wordpress.com/2019/07/52861-1pb5hxsxogjrnda_si4nj9q.png?w=775)\n",
    "*Ignore the hidden vectors predicting the padding tokens and only focus on the vectors that predict on the words*\n",
    "source: https://medium.com/@plusepsilon/the-bidirectional-language-model-1f3961d1fb27\n",
    "\n",
    "ELMo uses the Bi-directional Language Model to get a new embedding that will be concatenated with the initialized word embedding. Concretely, the word “are” in the above figure will have a representation formed with the following embedding vectors\n",
    "\n",
    "- Original embedding, GloVe, Word2Vec or FastText for example\n",
    "- Forward pass hidden layer representation vector\n",
    "- Backward pass hidden layer representation vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About ELMo Word Vectors: \n",
    "ELMo models both \n",
    "- (1) complex characteristics of word use (e.g., syntax and semantics)\n",
    "- (2) how these uses vary across linguistic contexts (i.e., to model polysemy)\n",
    "<br>\n",
    "<br>\n",
    "- ELMo `word vectors` are **learned functions of the internal states of a deep bidirectional language model (biLM)**, which is pretraind on a large text corpus\n",
    "<br>\n",
    "<br>\n",
    "- ELMo assigns each token/word **a representation that is function of the entire input sentence**\n",
    "<br>\n",
    "<br>\n",
    "- ELMo representations are **deep**, in the sense that they are **a function of all of the internal layers of the biLM**\n",
    "- In other words, ELMo doesn't just use the top LSTM layer, but all the internal layers\n",
    "<br>\n",
    "<br>\n",
    " - **higher-level LSTM states** capture **context-dependent aspects of word meaning**\n",
    " - **lower-level states** model aspects of **syntax**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ELMo does well in 6 diverse NLP tasks\n",
    "\n",
    "| Task | Description | Comments about Dataset | Evaluation Parameter | Previous SOTA | ELMo SOTA |\n",
    "| ------ |:------: |:------ |------ |------ | ------ | \n",
    "| SQuAD | Stanford Question Answering Dataset | a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable | F1 score (harmonic mean of precision and recall) | 84.4 |  85.8 |\n",
    "| SNLI | Stanford Natural Language Inference | SNLI corpus (version 1.0) is a collection of 570k human-written English sentence pairs manually labeled for balanced classification with the labels entailment, contradiction, and neutral, supporting the task of natural language inference (NLI), also known as recognizing textual entailment (RTE) | Accuracy | 88.6 | 88.7 |\n",
    "| SRL | Semantic Role Labeling | Semantic Role Labeling (SRL) recovers the latent predicate argument structure of a sentence, providing representations that answer basic questions about sentence meaning, including “who” did “what” to “whom,” etc | F1 Score | 81.7 | 84.6 |\n",
    "| Coref | Coreference resolution | Coreference resolution is the task of finding all expressions that refer to the same entity in a text. | Average F1 | 67.2 | 70.4 |\n",
    "| NER | Named Entity Recognition | The named entity recognition model identifies named entities (people, locations, organizations, and miscellaneous) in the input text | F1 | 91.93 | 92.22 |\n",
    "| SST-5 | 5-class Stanford Sentiment Treebank Dataset | fine-grained sentiment classification task uses 5 discrete classes: Strongly positive, Weakly positive, Neutral, Weakly negative, Strongly negative | Accuracy | 53.7 | 54.7 |\n",
    "<br>\n",
    "**sources for the Task Description:**\n",
    "- https://rajpurkar.github.io/SQuAD-explorer/\n",
    "- https://nlp.stanford.edu/projects/snli/\n",
    "- https://demo.allennlp.org/semantic-role-labeling/MTIzODQzNg==\n",
    "- https://demo.allennlp.org/coreference-resolution/MTIzODQzNA==\n",
    "- https://demo.allennlp.org/named-entity-recognition/MTIzODQzOA==\n",
    "- https://towardsdatascience.com/fine-grained-sentiment-analysis-in-python-part-1-2697bb111ed4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Bidirectional Language Models\n",
    "> Gist: ELMo word representations are functions of the entire input sentence **computed on top of two-layer biLMs**\n",
    "\n",
    "Given a sequence of N tokens $ (t_1,t_2,...,t_N) $, **a forward language model** computes the probability of the sequence by modeling the probability of\n",
    "$$ {\\prod\\limits_{k=1}^{N} {P(t_k,\\ |\\ {t_1, ...,t_{t-2},t_{t-1} })}}$$ \n",
    "\n",
    "A **backward LM** is similar to a forward LM, except it runs over the sequence in reverse, predicting the previous token given the future context:\n",
    "\n",
    "$$ {\\prod\\limits_{k=1}^{N} {P(t_k,\\ |\\ {t_{k+1},t_{k+2} , ...,t_{N-1},t_{N} })}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A biLM combines both a forward and backwardLM. Our formulation jointly maximizes the log likelihood of the forward and backward directions\n",
    "![](../images/ELMo_logliklihood.png)\n",
    "![](../images/ELMo_parameters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2. ELMo Embedding **\n",
    "> Gist: ELMo is a liner combination of the internal/hidden states (equation consisting of $\\gamma^{task}, s^{task} and \\ h_{k,j}^{LM}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ELMo embedding for each token is a combination of the intermediate layer representations in the biLM\n",
    "$$ R_k = \\{x_k^{LM}, \n",
    "\\overrightarrow{\n",
    "h_{k,j}^{LM}\n",
    "}, \n",
    "\\overleftarrow{\n",
    "h_{k,j}^{LM}\n",
    "} | \\ \n",
    "j = 1, 2, ...L\\} $$\n",
    "\n",
    "$$ R_k = \\{ h_{k,j}^{LM} \\ | \\ j = 0, 1, 2, ...L\\} $$\n",
    "\n",
    "where \n",
    "- $ x_{k,j}^{LM} $ - *context-independent* original embedding. For e.g.: GloVe, Word2Vec or FastText\n",
    "- $\\overrightarrow{\n",
    "h_{k,j}^{LM}\n",
    "} $ - context-dependent representation of the forward language model of the jth layer\n",
    "- `L` is the last LSTM layer\n",
    "- $ h_{k,0}^{LM} $ is the context independent token layer  representation\n",
    "- $ h_{k,j}^{LM} = [\n",
    "\\overrightarrow{\n",
    "h_{k,j}^{LM}\n",
    "},\n",
    "\\overleftarrow{\n",
    "h_{k,j}^{LM}\n",
    "}\n",
    "]\n",
    "$ \n",
    "\n",
    "ELMo combines all the layers of the biLM representation into a single vector $ELMo_k = E(R_k)$ where ELMo collapses all layers in `R` into a single vector  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While finetuning with respect to a task, the ELMo embedding for token `k` is\n",
    "![](../images/ELMo_finetuned_embeddings.png)\n",
    "where\n",
    "- $\\gamma^{task} $ = a scalar quantity = useful in optimization process\n",
    "- $ s^{task} $ = softmax-normalized weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 3. How to use ELMo in most supervised tasks in NLP**\n",
    "> Gist: Most supervised NLP models share a common architecture at the lowest layers, allowing us to add ELMo in a easy manner\n",
    "\n",
    "- Given a sequence of tokens $ (t_1,t_2,...,t_N) $, it is standard to form a context-independent token representation \n",
    "$$ x_k = pre-trained word embeddings +(optionally)character-based representations $$\n",
    "<br>\n",
    "<br>\n",
    "- concatenate the ELMo vector $ELMo_k^{task}$ with $ x_k $ to create\n",
    "$$ ELMo\\ enhanced\\ representation = [x_k;ELMo_k^{task}] $$\n",
    "\n",
    "- The `ELMo enhanced representation` is then used for downstream task like SQuAD, SNLI, SST-5, etc.,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Pre-trained Bidirectional LM Architecture**: \n",
    "![](../images/ELMo_pretrained_bidirectionalLM_architecture.png)\n",
    "- final model uses L = 2 biLSTM layers with 4096 units and 512 dimension projections and a residual connection from the first to second layer.\n",
    "- context-independent representation $ x_k $ uses 2048 character n-gram convolutional filters followed by two highway layers and a linear projection down to a 512 representation\n",
    "<br>\n",
    "<br>\n",
    "> source: \n",
    "> - What are highway layers: https://papers.nips.cc/paper/5850-training-very-deep-networks.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** \n",
    "- high-quality deep context-dependent representations are learned from biLMs\n",
    "- the biLM layers efficiently encode different types of syntactic and semantic information about words-in-context\n",
    "- using all layers improves overall task performance rather than just the top LSTM layer "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
