{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Main Sources of Reference: \n",
    ">  - Evolution of TL in NLP: https://arxiv.org/pdf/1910.07370v1.pdf\n",
    ">  - ULMFiT paper: https://arxiv.org/pdf/1801.06146.pdf\n",
    ">  - Coursera-Deeplearning.ai course on **Sequence Models** explains the formula used here for different RNN units\n",
    ">  - Yashu Seth on AWD LSTM - https://yashuseth.blog/2018/09/12/awd-lstm-explanation-understanding-language-model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolution of RNN architectures for Transfer Learning in NLP (Part 1)\n",
    "\n",
    "- The crux of Transfer Learning: <br> \n",
    " - (1) Build a Language Model^ that understands the underlying features of the text \n",
    " - (2) Fine-tune the Language Model with additional layers for downstream tasks\n",
    "> Why Language Model <br>\n",
    "> *Language modeling can be seen as the ideal source task as it captures many facets of language relevant for downstream tasks, such as long-term dependencies, hierarchical relations and sentiment* <br>\n",
    "> \n",
    ">Ruder et al in the ULMFiT paper\n",
    "_______________________________________________________________________________________________________________\n",
    "\n",
    "#### Agenda of Part 1\n",
    "- Introduction to Language Modeling\n",
    "- How Transfer Learning Evolved\n",
    "- Evolution of RNN units - RNN, LSTM, GRU, AWD-LSTM\n",
    "\n",
    "#### Covered in Part 2\n",
    "- ULMFiT\n",
    "\n",
    "#### Covered in Part 3\n",
    "- ELMo\n",
    "_______________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Language Modeling\n",
    "\n",
    "Language Model: **A model of the probability of a sequence of words**\n",
    "\n",
    "- A language model can assign probability to each possible next word. And also, help in assigning a probability to an entire sentence. \n",
    "\n",
    "#### Applications of Language Model\n",
    "\n",
    "* Speech Recognition: E.g.: P('recognize speech') >> P('wreck a nice beach')\n",
    "* Spelling Correction: E.g.: P('I have a gub') << P('I have a gun')\n",
    "* Machine Translation: E.g.: P('strong winds') > P('large winds')\n",
    "* Optical Character Recognition/ Handwriting Recognition\n",
    "* Autoreply Suggestions\n",
    "* Text Classification (discussed with python implementation of a simple N-gram model)\n",
    "* Text Generation (discussed this with Char-level and Word-level language models)\n",
    "_______________________________________________________________________________________________________________\n",
    "\n",
    "#### Evaluation Metrics for LM\n",
    "\n",
    "##### (1) Perplexity\n",
    "- Perplexity is a measurement of how well a probability model predicts a sample\n",
    "- It is used to compare probability models\n",
    "- A low perplexity indicates the probability distribution is good at predicting the sample  \n",
    "\n",
    "Definition: \n",
    "- Perplexity is the inverse probability of  the test set, normalized by the number  of words. <br>\n",
    "Perplexity of test data = PP(test data) =\n",
    "\n",
    "$$ P(w_1,\\ w_2,\\ ....,\\ w_N)^{1 \\over N}  $$\n",
    "\n",
    "$$ {\\sqrt[N]{1 \\over P(w_1,\\ w_2,\\ ....,\\ w_N)}} $$\n",
    "\n",
    "$$ {\\sqrt[N]{\\prod\\limits_{i=1}^{N} {1 \\over P(w_i,\\ |\\ {w_{i-1},\\ ....,\\ w_1})}}}$$ \n",
    "\n",
    "**the lower the perplexity, the lower is the entropy in the generated text, the better is the model**\n",
    "\n",
    "_______________________________________________________________________________________________________________\n",
    "\n",
    "##### (2) Log-likelihood\n",
    "\n",
    "###### Before that, what is Markov Assumption?  \n",
    "\n",
    "So the probability of occurrence of a 4-word sentence: \n",
    "\n",
    "$$ Prob \\ ({ A,\\ B,\\ C,\\ D}) = Prob\\ ({ A_{before}\\ B_{before}\\ C_{before}\\ D}) = Prob\\ ( {A \\ |\\ {B,\\ C,\\ D}} ) \\  \\times \\  Prob\\ ( {B \\ |\\ {C,\\ D}})\\ \\times \\ Prob\\ ({C \\ |\\ D})\\ \\times \\ Prob\\ (D) $$\n",
    "\n",
    "\n",
    "\n",
    "> “What I see NOW depends only on what I saw in the PREVIOUS step”\n",
    "\n",
    "$$ P(w_i,\\ |\\ {w_{i-1},\\ ....,\\ w_1}) = P(w_i,\\ |\\ {w_{i-1}}) $$\n",
    "\n",
    "Hence the probability of occurrence of a 4-word sentence with Markov Assumption is:\n",
    "\n",
    "$$ Prob \\ ({ A,\\ B,\\ C,\\ D}) = Prob\\ ({ A_{before}\\ B_{before}\\ C_{before}\\ D}) = Prob\\ ( {A \\ |\\ {B}} ) \\  \\times \\  Prob\\ ( {B \\ |\\ {C}})\\ \\times \\ Prob\\ ({C \\ |\\ D})\\ \\times \\ Prob\\ (D) $$\n",
    "\n",
    "Based on Markov Assumption, the log-likelihood of a N-word sentence:\n",
    "_________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\log P(w_1,\\ w_2,\\ ....,\\ w_N) = \\sum\\limits_{\\ i\\ =\\ 2}^{N} {\\log P(w_i\\ |\\ w_{i-1})}\\ +\\ \\log P(w_1) $$ \n",
    "\n",
    "- where N  is the number of words in the sentence. \n",
    "- Since log (probabilities) are always negative (see graph), shorter sentences will have higher probability of occurrence. \n",
    "\n",
    "**To normalize it**,\n",
    "$$ {1 \\over N} \\log P(w_1,\\ w_2,\\ ....,\\ w_N) = {1 \\over N} \\left [\\ \\sum\\limits_{\\ i\\ =\\ 2}^{N} {\\log P(w_i\\ |\\ w_{i-1})}\\ +\\ \\log P(w_1) \\right] $$ \n",
    "\n",
    "**Higher the log probability value, better is the model **\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison between Perplexity and Log-Probability metrics for a Bigram Markov Language Model\n",
    "\n",
    "$$ Perplexity\\ for\\ a\\ bigram\\ model = {\\sqrt[N]{\\prod\\limits_{i=1}^{N} {1 \\over P(w_i,\\ |\\ {w_{i-1}})}}} $$\n",
    "\n",
    "Comparing with normalized log probability for a bigram model:\n",
    "\n",
    "$$ LogProbability\\ for\\ a\\ bigram\\ model = {1 \\over N} \\left [\\ \\sum\\limits_{\\ i\\ =\\ 2}^{N} {\\log P(w_i\\ |\\ w_{i-1})}\\ +\\ \\log P(w_1) \\right] $$ \n",
    "\n",
    "___________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Transfer Learning Evolved\n",
    "\n",
    "- Stage1: NLP started with rule-based and statistical methodologies\n",
    "- Stage2: ML algos such as Naive Bayes, SVM, Trees coupled with bag-of-words word representations\n",
    "- Stage3: Recurrent Neural Networks such as LSTM\n",
    "- Stage4: Seq2Seq Architectures and Transformers --> 'ImageNet' moment in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Transformer and Language Models](../images/TL and LM Flow_chart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolution of RNN units\n",
    "\n",
    "Why RNNs came into existence?\n",
    "- Models such as the **Multi-layer Perceptron Network, vector machines and logistic regression** did not perform well on sequence modelling tasks (e.g.: text_sequence2sentiment_classification)\n",
    "- Why? **Lack of memory element** ; **No information retention**\n",
    "\n",
    "#### Cometh the RNNs: \n",
    "\n",
    "- RNNs attempted to redress this shortcoming by introducing loops within the network, thus allowing the retention of information.\n",
    "\n",
    "**An unrolled RNN**\n",
    "![An un-rolled RNN Cell](../images/an_unrolled_RNN.png)\n",
    "\n",
    "**Digging Deeper - tanh activations**\n",
    "![](../images/RNN-units.png)\n",
    "\n",
    "**Mathematically, a single RNN cell**\n",
    "![](../images/maths_RNN_fwd_prop.png)\n",
    "\n",
    "**Mathematically, a simple RNN network**\n",
    "![](../images/maths_RNN_units.png)\n",
    "\n",
    "##### How are weights such as $W_{aa}, W_{ax}, b_a$ computed\n",
    "- BPTT: **Back Propagation Through Time**\n",
    "![](../images/rnn_cell_backprop.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantage of a vanilla RNN:\n",
    "- Better than traditional ML algos in retaining information\n",
    "\n",
    "#### Limitations of a vanilla RNN:\n",
    "- RNNs fail to model long term dependencies.\n",
    " - the information was often **\"forgotten\"** after the unit activations were multiplied several times by small numbers\n",
    "- Vanishing gradient and exploding gradient problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Long Short Term Memory (LSTM): \n",
    "- a special type of RNN architecture\n",
    "- designed to keep information retained for extended number of timesteps\n",
    "- each LSTM cell consists of 4 layers (3 sigmoid and 1 tanh)\n",
    "\n",
    "![](../images/LSTM.png)\n",
    "<caption><center>LSTM-cell. This tracks and updates a \"cell state\" or memory variable $c^{\\langle t \\rangle}$ at every time-step, which can be different from $a^{\\langle t \\rangle}$. </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the gates\n",
    "\n",
    "#### - Forget gate\n",
    "\n",
    "For the sake of this illustration, lets assume we are reading words in a piece of text, and want use an LSTM to keep track of grammatical structures, such as whether the subject is singular or plural. If the subject changes from a singular word to a plural word, we need to find a way to get rid of our previously stored memory value of the singular/plural state. In an LSTM, the forget gate lets us do this: \n",
    "\n",
    "$$\\Gamma_f^{\\langle t \\rangle} = \\sigma(W_f[a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}] + b_f)\\tag{1} $$\n",
    "\n",
    "Here, $W_f$ are weights that govern the forget gate's behavior. We concatenate $[a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}]$ and multiply by $W_f$. The equation above results in a vector $\\Gamma_f^{\\langle t \\rangle}$ with values between 0 and 1. This forget gate vector will be multiplied element-wise by the previous cell state $c^{\\langle t-1 \\rangle}$. So if one of the values of $\\Gamma_f^{\\langle t \\rangle}$ is 0 (or close to 0) then it means that the LSTM should remove that piece of information (e.g. the singular subject) in the corresponding component of $c^{\\langle t-1 \\rangle}$. If one of the values is 1, then it will keep the information. \n",
    "\n",
    "#### - Update gate\n",
    "\n",
    "Once we forget that the subject being discussed is singular, we need to find a way to update it to reflect that the new subject is now plural. Here is the formulat for the update gate: \n",
    "\n",
    "$$\\Gamma_u^{\\langle t \\rangle} = \\sigma(W_u[a^{\\langle t-1 \\rangle}, x^{\\{t\\}}] + b_u)\\tag{2} $$ \n",
    "\n",
    "Similar to the forget gate, here $\\Gamma_u^{\\langle t \\rangle}$ is again a vector of values between 0 and 1. This will be multiplied element-wise with $\\tilde{c}^{\\langle t \\rangle}$, in order to compute $c^{\\langle t \\rangle}$.\n",
    "\n",
    "#### - Updating the cell \n",
    "\n",
    "To update the new subject we need to create a new vector of numbers that we can add to our previous cell state. The equation we use is: \n",
    "\n",
    "$$ \\tilde{c}^{\\langle t \\rangle} = \\tanh(W_c[a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}] + b_c)\\tag{3} $$\n",
    "\n",
    "Finally, the new cell state is: \n",
    "\n",
    "$$ c^{\\langle t \\rangle} = \\Gamma_f^{\\langle t \\rangle}* c^{\\langle t-1 \\rangle} + \\Gamma_u^{\\langle t \\rangle} *\\tilde{c}^{\\langle t \\rangle} \\tag{4} $$\n",
    "\n",
    "\n",
    "#### - Output gate\n",
    "\n",
    "To decide which outputs we will use, we will use the following two formulas: \n",
    "\n",
    "$$ \\Gamma_o^{\\langle t \\rangle}=  \\sigma(W_o[a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}] + b_o)\\tag{5}$$ \n",
    "$$ a^{\\langle t \\rangle} = \\Gamma_o^{\\langle t \\rangle}* \\tanh(c^{\\langle t \\rangle})\\tag{6} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantage of a LSTM:\n",
    "- Better equipped for long range dependencies\n",
    "\n",
    "#### Limitations of LSTM:\n",
    "- resists better than LSTM for vanishing gradient problem\n",
    "- Added gates lead to more computation requirement and LSTMs tend to be slower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gated Recurrent Unit\n",
    "- a curtailed version of LSTM\n",
    "- retains the resisting vanishing gradient properties of LSTM but GRUs are internally simpler and faster than LSTMs.\n",
    "> `forget` and `update` gates are merged into a single `update` gate\n",
    "> The update gate decides how much of previous memory to keep around.\n",
    ">\n",
    "> The `reset` gate defines how to combine new input with previous value.\n",
    "\n",
    "![](https://cdn-images-1.medium.com/freeze/max/1000/1*dhq14CzJijlqjf7IlDB0uw.png?q=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of performance between GRU and LSTM:\n",
    "- GRUs are almost on par with LSTM but with efficient computation. \n",
    "- However, with large data LSTMs with higher expressiveness may lead to better results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
