{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Main Sources of Reference: \n",
    ">  - Evolution of TL in NLP: https://arxiv.org/pdf/1910.07370v1.pdf\n",
    ">  - ULMFiT paper: https://arxiv.org/pdf/1801.06146.pdf\n",
    ">  - Articles on ULMFiT\n",
    ">   - https://humboldt-wi.github.io/blog/research/information_systems_1819/group4_ulmfit/#vlbs\n",
    ">   - https://medium.com/@zhangguanguan1/an-application-of-universal-language-model-fine-tuning-to-the-classification-of-multiclass-company-e77527e2bcae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolution of RNN architectures for Transfer Learning in NLP (Part 2)\n",
    "\n",
    "#### Already covered in Part 1\n",
    "- Introduction to Language Modeling\n",
    "- How Transfer Learning Evolved\n",
    "- Evolution of RNN units - RNN, LSTM, GRU, AWD-LSTM\n",
    "\n",
    "#### Agenda covered here in Part 2\n",
    "- ULMFiT\n",
    "\n",
    "#### Agenda to be covered here in Part 3\n",
    "- ELMo\n",
    "_______________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why ULMFiT became successful:**\n",
    "\n",
    "Historically: \n",
    "    - Fine-tuning a LM required millions of in-domain corpus (in other words, transfer learning was not possible). Hence limited applicability <br>\n",
    "    - LMs overfit to small datasets and suffered catastrophic forgetting when fine-tuned with a classifier <br>\n",
    "    - ULMFiT used a very common 3 layer LSTM architecture (ignoring encoder, decoder and fine-tuning layers) but used a variety of novel training techniques to make the the concept of `inductive transfer learning` (pre-train in a huge generic corpus and fine-tune for a target data) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ULMFiT\n",
    "- Universal Language Model Fine-tuning (ULMFiT) for Text Classification\n",
    " - This paper introduces techniques that are essential to fine-tune an LSTM-based Language Model\n",
    " - This paper specifically the superior performance of ULMFiT approach  in 6 text classification datasets\n",
    " - The performance was measured in terms of error rates\n",
    "   ![](../images/ULMFiT_table.png)\n",
    " - The 6 text classification datasets used are: \n",
    "  ![](../images/ULMFiT_6_data_table.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does ULMFiT propose?\n",
    "- Pretrain a LM on a large general-domain corpus and fine-tune it on the target task using novel* techniques\n",
    "- Why called **Universal** (the following have become synonymous with what a TL model is):\n",
    " - 1) It works across tasks varying in document size, number, and label type\n",
    " - 2) it uses a single architecture and training process; \n",
    " - 3) it requires no custom feature engineering or preprocessing; and \n",
    " - 4) it does not require additional in-domain documents or labels\n",
    "- What are the **novel** techniques: \n",
    " - discriminative fine-tuning,\n",
    " - slanted triangular learning rates, and \n",
    " - gradual unfreezing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison Notes with CV and other NLP Works\n",
    "- Compared to CV models (which are several layers deep), **NLP models are typically more shallow** and thus require different fine-tuning methods\n",
    "- Features in deep neural networks in CV have been observed to transition **from general to task-specific** from the **first to the last layer**. \n",
    "- For this reason, most work in CV focuses on transferring the first layers of the model and fine-tuning the last or several of the last layers and leaving the remaining layers frozen\n",
    "- **Hypercolumns**:\n",
    " - CV: A hypercolumn at a pixel in CV is the vector of all activations of CNN units above that pixel\n",
    " - NLP: Concatenation of embeddings at different layers in a pretrained model\n",
    " - In CV, hypercolumns have been nearly entirely superseded by end-to-end fine-tuning\n",
    "- **Multi-task learning**\n",
    " - One of the papers cited mentions training a LM objective jointly with the main task objective\n",
    " \n",
    "\n",
    ">> #### ULMFiT does not use any of the custom engineered architectures (needed for hyper columns), no residual network, no multi-task learning objectives. But performs better than the above methodologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ULMFiT uses AWD-LSTM cell based Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About AWD LSTM\n",
    "- Average SGD Weight Dropped (AWD) LSTM\n",
    "- It uses `DropConnect` and a variant of Average-SGD (`NT-ASGD`) along with several other well-known regularization strategies\n",
    "\n",
    "**Why `dropout` won't work?**\n",
    " - Dropout, an algorithm that randomly(with a probability p) ignore units’ activations during the training phase allows for the regularization of a neural network.\n",
    " - By diminishing the probability of neurons developing inter-dependencies, it increases the individual power of a neuron and thus reduces overfitting. \n",
    " - However, dropout inhibits the RNNs capability of developing long term dependencies as there is loss of information caused due to randomly ignoring units activations.\n",
    "\n",
    "**Hence `drop connect`**\n",
    "- the drop connect algorithm randomly drops weights instead of neuron activations. It does so by randomly(with probability 1-p) setting weights of the neural network to zero during the training phase. \n",
    "- Thus **redressing the issue of information loss** in the Recurrent Neural Network **while still performing regularization.**\n",
    "\n",
    "![](https://yashuseth.files.wordpress.com/2018/09/nn_do1.jpg?w=685)\n",
    "\n",
    "**What is NT-ASGD**\n",
    "- Non-monotonically Triggered Average Stochastic Gradient\n",
    "- For Language Modeling Tasks, **traditional SGD without momentum outperforms other algorithms** such as momentum SGD, Adam, Adagrad, and RMSProp\n",
    "- ASGD -- a variant of the traditional SGD algorithm\n",
    "\n",
    "*Batch GD:*\n",
    "```python\n",
    "for i in range(nb_epochs):\n",
    "  params_grad = evaluate_gradient(loss_function, data, params)\n",
    "  params = params - learning_rate * params_grad\n",
    "```\n",
    "\n",
    "*Traditional SGD:*\n",
    "```python\n",
    "for i in range(nb_epochs):\n",
    "  np.random.shuffle(data)\n",
    "  for example in data:\n",
    "    params_grad = evaluate_gradient(loss_function, example, params)\n",
    "    params = params - learning_rate * params_grad\n",
    "```\n",
    "\n",
    "*Average Traditional SGD:*\n",
    "```python\n",
    "for i in range(nb_epochs):\n",
    "  np.random.shuffle(data)\n",
    "  params_list = []  \n",
    "  for index, example in enumerate(data):\n",
    "    avg_fact = 1 / max(index - K, 1) #when index > K, max(index-K, 1) will be index-K\n",
    "    if avg_fact == 1: #when index < K\n",
    "        params_grad = evaluate_gradient(loss_function, example, params)\n",
    "        params = params - learning_rate * params_grad\n",
    "    else: # when index > K\n",
    "        params_list.append(params)\n",
    "        params = avg_fact * (sum(params_list) + (params - learning_rate * params_grad))\n",
    "```\n",
    "\n",
    "- `K` is the minimum number of iterations run before weight averaging starts. \n",
    "- So before K iterations, the ASGD will behave similarly to a traditional SGD.\n",
    "- `index` is the current number of iterations done, \n",
    "- `sum(params_list)` is the sum of weights from iteration K to index and \n",
    "- `learning_rate` is the learning rate at iteration t decided by a learning rate scheduler.\n",
    "\n",
    "\n",
    "#### What is NT ASGD then?\n",
    "- A commonly used strategy while using the SGD optimizer is to reduce the learning rate by a fixed quantity when the validation error worsens. \n",
    "- The Nonmonotonically triggered ASGD employs a similar technique.\n",
    "- It differs in the fact that, instead of performing averaging when the validation error worsens NT-ASGD performs the averaging operation if the validation error fails to improve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ULMFiT language Model Architecture with an example (not considering the two blocks of classification layer)**\n",
    "![](../images/ULMFiT_Language_Model.png \"source: https://medium.com/@zhangguanguan1/an-application-of-universal-language-model-fine-tuning-to-the-classification-of-multiclass-company-e77527e2bcae\")\n",
    "\n",
    "**Transforming the language model into a Classifier**\n",
    "![](https://miro.medium.com/max/1201/1*TxWWBKy4ot7jq-lnJfMJUA.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Stages of ULMFiT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/ULMFiT_pretraining.png)\n",
    "\n",
    "1.LM Pre-training: LM is trained on **a general-domain corpus** to capture general features of the language in different layers\n",
    "\n",
    "- Capture the general properties of the given language through pretraining: Pre-trained their language model on **WikiText-103** – a large general-purpose dataset that consists of 28,595 preprocessed articles and 103 million words.\n",
    "- Computationally expensive | need to be performed only once.\n",
    "\n",
    "![](../images/ULMFiT_finetuning.png)\n",
    "\n",
    "2.LM fine-tuning: full LM is fine-tuned on **target task data** using discriminative fine-tuning (‘Discr’) and slanted triangular learning rates (STLR) to learn task-specific features\n",
    "\n",
    "- Finetune the LM to capture the inherent nuances of the **target task**\n",
    "- Performed on relatively smaller dataset | requires less computation power\n",
    "- Finetuning using two novel techniques: \n",
    " - **Discriminative Finetuning**\n",
    " - **Slanted Triangular Learning Rates**\n",
    "\n",
    "![](../images/ULMFiT_classifier-fine-tuning.png)\n",
    "\n",
    "3.The classifier is fine-tuned on the target task using gradual unfreezing, ‘Discr’, and STLR to preserve low-level representations and adapt high-level ones (shaded: unfreezing stages; black: frozen)\n",
    "\n",
    "- To perform task-specific classification, **two linear blocks** initialized from scratch are added to the language model. \n",
    "- (similar to the practice in CV classifiers) each block uses\n",
    " - **batch normalization**\n",
    " - **dropout**\n",
    " - **ReLU Activations**\n",
    "In the last layer after these two linear blocks <br>\n",
    " - **a softmax layer**\n",
    "- Parameters in these **target task-specific classifier layers** are the only ones that are learned from scratch\n",
    "- The first linear layer takes as the input the pooled last layer hidden states\n",
    "- Techniques used in Classifier Finetuning:\n",
    " - **Concat pooling**\n",
    " - **Gradual Unfreezing**\n",
    " - **BPTT for Text Classification (BP3TC)**\n",
    " - **Bidirectional language model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Novel Techniques - Part 1: \n",
    "\n",
    "#### Used in Language Model FineTuning\n",
    "\n",
    "##### <center>**Discriminative fine-tuning**:</center>\n",
    "- Core Idea: different layers capture different types of information, hence should be fine-tuned to different extents\n",
    "\n",
    "Stochastic Gradient Descent of a model’s parameters $\\theta$ at time step `t`:\n",
    "$$ \\theta_t = \\theta_{t-1} - \\eta \\cdot \\nabla_\\theta J( \\theta) $$\n",
    "\n",
    "where\n",
    "- $\\eta$ is the learning rate\n",
    "- $\\nabla_\\theta J( \\theta)$ gradient with regard to the model’s objective function\n",
    "\n",
    "For discriminative fine-tuning, we \n",
    "- split the parameter $\\theta$ into {$\\theta^1$,...,$\\theta^L$} where\n",
    " - $\\theta^l$ contains the parameters of the model at the $l^{th}$ layer and L is the number of layers of the model (here L =3)\n",
    "\n",
    "- split the parameter $\\eta$ into {$\\eta^1$,...,$\\eta^L$} where\n",
    " - $\\eta^1$ is the learning rate for $l^{th}$  layer\n",
    " \n",
    "**SGD with discriminative finetuning**:\n",
    "$$ \\theta_t^l = \\theta_{t-1}^l - \\eta^l \\cdot \\nabla_\\theta^l J( \\theta) $$\n",
    "\n",
    "Choose the learning rate $\\eta^L$ for the last layer and then compute learning rates of the previous layers using\n",
    "$$ \\eta^{l-1} = { \\eta^{l}\\over 2.6 } $$ \n",
    "\n",
    "**How to choose $\\eta^L$ **\n",
    "**Slanted triangular learning rates**:\n",
    "\n",
    "**Objective**: \n",
    "- Want the model to quickly converge to a suitable region of the parameter space in the beginning of training and then refine its parameters <br>\n",
    "*How it won't be achieived?*\n",
    "- Using the same learning rate (LR) or an annealed learning rate (gradually reducing) throughout training\n",
    "\n",
    "*How it can be achieved?*\n",
    "- **STLR**\n",
    " - first linearly increase the learning rate and then linearly decays it\n",
    " \n",
    " ![](../images/ULMFiT_STLR.png)\n",
    " ![](../images/ULMFiT_STLR_equations.png)\n",
    "\n",
    "where \n",
    "- `T` is the number of iterations\n",
    "- `cut_frac` is the fraction of iterations we increase the LR\n",
    "- `cut` is the iteration at which we switch from increasing to decreasing the LR \n",
    "- `p` is the fraction of the number of iterations we have increased or will decrease the LR respectively \n",
    "- `ratio` = $\\eta_{min}\\over\\eta_{max}$ specifies how much smaller the lowest LR is from the maximum LR $\\eta_{max}$\n",
    "- $\\eta_t$ is the learning rate at iteration t\n",
    "- `cut_frac = 0.1` , `ratio = 32` and $\\eta_{max} = 0.01$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Novel Techniques - Part 2: \n",
    "\n",
    "#### Classifier FineTuning\n",
    "\n",
    "**Concat Pooling**:\n",
    "- As input documents can consist of hundreds of words, information may get lost if we only consider the last hidden state of the model\n",
    "$$h_c = [h_T, maxpool(H), meanpool(H)]$$\n",
    "where \n",
    "$$ H = {h_1,...,h_T} $$\n",
    "$ [] $$ is the concatenation $\n",
    "$$ T is the last time step $$\n",
    "\n",
    "**Gradual Freezing**:\n",
    "- Overly aggressive fine-tuning will cause catastrophic forgetting, eliminating the benefit of the information captured through language modeling\n",
    "- too cautious fine-tuning will lead to slow convergence (and resultant overfitting)\n",
    "- Besides discriminative finetuning and triangular learning rates, we propose gradual unfreezing for fine-tuning the classifier\n",
    "\n",
    "Idea: \n",
    "- Rather than fine-tuning all layers at once, which risks catastrophic forgetting, we propose to gradually unfreeze the model starting from the last layer as this contains the least general knowledge\n",
    "- We first unfreeze the last layer and fine-tune all unfrozen layers for one epoch\n",
    "- We then unfreeze the next lower frozen layer and repeat, until we finetune all layers until convergence at the last iteration\n",
    "![](../images/ULMFiT_gradual_unfreeze.png)\n",
    "\n",
    "\n",
    "**Back Propagation through Time for Text Classification (BP#TC)**:\n",
    "\n",
    "**Simple BPTT (for RNNs)**\n",
    "![](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcTF6q2J0fxaYPaBKBK1AvL791-ztNyR-2KwSZmDFAsWp-TxUIJv)\n",
    "- The red line indicates gradient propagation through time in the reverse direction\n",
    "- Language models are trained with backpropagation through time (BPTT) to enable gradient propagation for large input sequences\n",
    "- **Why BP3TC** - In order to make fine-tuning a classifier for large documents feasible, authors of ULMFiT proposed BPTT for Text Classification (BPT3C)\n",
    "\n",
    "**Bidirectional LM**:\n",
    "- Train both a forward LM as well as a backward LM. Consequently averaging the predictions given by both the Language Models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
