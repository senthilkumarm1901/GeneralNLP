Before attempting the best-in-class Language models from the complex LSTM stable, I wanted to take baby steps to understand the fundamentals better
That is the reason behind developing codes and notes for Language Models created before the Word2Vec era. 
I will add more notes and codes on new SOTA pre-trained language models that are beyond traditional word embeddings (difference mainly - provides contextualised word embeddings). These pre-trained LMs enter the territory of transfer learning. 
