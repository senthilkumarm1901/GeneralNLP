{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.2.1'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format(nltk.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tokenization__\n",
    "1. Tokenizing Text into sentences\n",
    "2. Tokenizing Text into words\n",
    "3. Tokenizing Text by Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components (or sometimes, principal modes of variation). The number of principal components is less than or equal to the smaller of the number of original variables or the number of observations. This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components. The resulting vectors are an uncorrelated orthogonal basis set. PCA is sensitive to the relative scaling of the original variables.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components (or sometimes, principal modes of variation). The number of principal components is less than or equal to the smaller of the number of original variables or the number of observations. This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components. The resulting vectors are an uncorrelated orthogonal basis set. PCA is sensitive to the relative scaling of the original variables.\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Sent_tokenizer=sent_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components (or sometimes, principal modes of variation).'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sent_tokenizer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word_tokenizer=[]\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "for each in Sent_tokenizer:\n",
    "    word_tokenizer.append(WordPunctTokenizer().tokenize(each))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word_tokenizer=[]\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "Word_Tokenizing=WordPunctTokenizer()\n",
    "for each in Sent_tokenizer:\n",
    "    word_tokenizer.append(Word_Tokenizing.tokenize(each))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Principal',\n",
       "  'component',\n",
       "  'analysis',\n",
       "  '(',\n",
       "  'PCA',\n",
       "  ')',\n",
       "  'is',\n",
       "  'a',\n",
       "  'statistical',\n",
       "  'procedure',\n",
       "  'that',\n",
       "  'uses',\n",
       "  'an',\n",
       "  'orthogonal',\n",
       "  'transformation',\n",
       "  'to',\n",
       "  'convert',\n",
       "  'a',\n",
       "  'set',\n",
       "  'of',\n",
       "  'observations',\n",
       "  'of',\n",
       "  'possibly',\n",
       "  'correlated',\n",
       "  'variables',\n",
       "  'into',\n",
       "  'a',\n",
       "  'set',\n",
       "  'of',\n",
       "  'values',\n",
       "  'of',\n",
       "  'linearly',\n",
       "  'uncorrelated',\n",
       "  'variables',\n",
       "  'called',\n",
       "  'principal',\n",
       "  'components',\n",
       "  '(',\n",
       "  'or',\n",
       "  'sometimes',\n",
       "  ',',\n",
       "  'principal',\n",
       "  'modes',\n",
       "  'of',\n",
       "  'variation',\n",
       "  ').'],\n",
       " ['The',\n",
       "  'number',\n",
       "  'of',\n",
       "  'principal',\n",
       "  'components',\n",
       "  'is',\n",
       "  'less',\n",
       "  'than',\n",
       "  'or',\n",
       "  'equal',\n",
       "  'to',\n",
       "  'the',\n",
       "  'smaller',\n",
       "  'of',\n",
       "  'the',\n",
       "  'number',\n",
       "  'of',\n",
       "  'original',\n",
       "  'variables',\n",
       "  'or',\n",
       "  'the',\n",
       "  'number',\n",
       "  'of',\n",
       "  'observations',\n",
       "  '.'],\n",
       " ['This',\n",
       "  'transformation',\n",
       "  'is',\n",
       "  'defined',\n",
       "  'in',\n",
       "  'such',\n",
       "  'a',\n",
       "  'way',\n",
       "  'that',\n",
       "  'the',\n",
       "  'first',\n",
       "  'principal',\n",
       "  'component',\n",
       "  'has',\n",
       "  'the',\n",
       "  'largest',\n",
       "  'possible',\n",
       "  'variance',\n",
       "  '(',\n",
       "  'that',\n",
       "  'is',\n",
       "  ',',\n",
       "  'accounts',\n",
       "  'for',\n",
       "  'as',\n",
       "  'much',\n",
       "  'of',\n",
       "  'the',\n",
       "  'variability',\n",
       "  'in',\n",
       "  'the',\n",
       "  'data',\n",
       "  'as',\n",
       "  'possible',\n",
       "  '),',\n",
       "  'and',\n",
       "  'each',\n",
       "  'succeeding',\n",
       "  'component',\n",
       "  'in',\n",
       "  'turn',\n",
       "  'has',\n",
       "  'the',\n",
       "  'highest',\n",
       "  'variance',\n",
       "  'possible',\n",
       "  'under',\n",
       "  'the',\n",
       "  'constraint',\n",
       "  'that',\n",
       "  'it',\n",
       "  'is',\n",
       "  'orthogonal',\n",
       "  'to',\n",
       "  'the',\n",
       "  'preceding',\n",
       "  'components',\n",
       "  '.'],\n",
       " ['The',\n",
       "  'resulting',\n",
       "  'vectors',\n",
       "  'are',\n",
       "  'an',\n",
       "  'uncorrelated',\n",
       "  'orthogonal',\n",
       "  'basis',\n",
       "  'set',\n",
       "  '.'],\n",
       " ['PCA',\n",
       "  'is',\n",
       "  'sensitive',\n",
       "  'to',\n",
       "  'the',\n",
       "  'relative',\n",
       "  'scaling',\n",
       "  'of',\n",
       "  'the',\n",
       "  'original',\n",
       "  'variables',\n",
       "  '.']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Is', 'this', 'good009']\n"
     ]
    }
   ],
   "source": [
    "text=\"Is' this good009 @@@ !!! $?\"\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "Regexp_tokenizing=RegexpTokenizer(\"\\w+\")\n",
    "RegEx_Tokenized_Words=[]\n",
    "RegEx_Tokenized_Words=Regexp_tokenizing.tokenize(text)\n",
    "print RegEx_Tokenized_Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Principal',\n",
       " 'component',\n",
       " 'analysis',\n",
       " 'PCA',\n",
       " 'is',\n",
       " 'a',\n",
       " 'statistical',\n",
       " 'procedure',\n",
       " 'that',\n",
       " 'uses',\n",
       " 'an',\n",
       " 'orthogonal',\n",
       " 'transformation',\n",
       " 'to',\n",
       " 'convert',\n",
       " 'a',\n",
       " 'set',\n",
       " 'of',\n",
       " 'observations',\n",
       " 'of',\n",
       " 'possibly',\n",
       " 'correlated',\n",
       " 'variables',\n",
       " 'into',\n",
       " 'a',\n",
       " 'set',\n",
       " 'of',\n",
       " 'values',\n",
       " 'of',\n",
       " 'linearly',\n",
       " 'uncorrelated',\n",
       " 'variables',\n",
       " 'called',\n",
       " 'principal',\n",
       " 'components',\n",
       " 'or',\n",
       " 'sometimes',\n",
       " 'principal',\n",
       " 'modes',\n",
       " 'of',\n",
       " 'variation',\n",
       " 'The',\n",
       " 'number',\n",
       " 'of',\n",
       " 'principal',\n",
       " 'components',\n",
       " 'is',\n",
       " 'less',\n",
       " 'than',\n",
       " 'or',\n",
       " 'equal',\n",
       " 'to',\n",
       " 'the',\n",
       " 'smaller',\n",
       " 'of',\n",
       " 'the',\n",
       " 'number',\n",
       " 'of',\n",
       " 'original',\n",
       " 'variables',\n",
       " 'or',\n",
       " 'the',\n",
       " 'number',\n",
       " 'of',\n",
       " 'observations',\n",
       " 'This',\n",
       " 'transformation',\n",
       " 'is',\n",
       " 'defined',\n",
       " 'in',\n",
       " 'such',\n",
       " 'a',\n",
       " 'way',\n",
       " 'that',\n",
       " 'the',\n",
       " 'first',\n",
       " 'principal',\n",
       " 'component',\n",
       " 'has',\n",
       " 'the',\n",
       " 'largest',\n",
       " 'possible',\n",
       " 'variance',\n",
       " 'that',\n",
       " 'is',\n",
       " 'accounts',\n",
       " 'for',\n",
       " 'as',\n",
       " 'much',\n",
       " 'of',\n",
       " 'the',\n",
       " 'variability',\n",
       " 'in',\n",
       " 'the',\n",
       " 'data',\n",
       " 'as',\n",
       " 'possible',\n",
       " 'and',\n",
       " 'each',\n",
       " 'succeeding',\n",
       " 'component',\n",
       " 'in',\n",
       " 'turn',\n",
       " 'has',\n",
       " 'the',\n",
       " 'highest',\n",
       " 'variance',\n",
       " 'possible',\n",
       " 'under',\n",
       " 'the',\n",
       " 'constraint',\n",
       " 'that',\n",
       " 'it',\n",
       " 'is',\n",
       " 'orthogonal',\n",
       " 'to',\n",
       " 'the',\n",
       " 'preceding',\n",
       " 'components',\n",
       " 'The',\n",
       " 'resulting',\n",
       " 'vectors',\n",
       " 'are',\n",
       " 'an',\n",
       " 'uncorrelated',\n",
       " 'orthogonal',\n",
       " 'basis',\n",
       " 'set',\n",
       " 'PCA',\n",
       " 'is',\n",
       " 'sensitive',\n",
       " 'to',\n",
       " 'the',\n",
       " 'relative',\n",
       " 'scaling',\n",
       " 'of',\n",
       " 'the',\n",
       " 'original',\n",
       " 'variables']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RegEx_Tokenized_Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'string599'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#just checking the RegEx\n",
    "import re\n",
    "re.search(r\"[\\w']+\",r\"string599\",re.I).group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Stop Words Removal__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'i',\n",
       " u'me',\n",
       " u'my',\n",
       " u'myself',\n",
       " u'we',\n",
       " u'our',\n",
       " u'ours',\n",
       " u'ourselves',\n",
       " u'you',\n",
       " u'your',\n",
       " u'yours',\n",
       " u'yourself',\n",
       " u'yourselves',\n",
       " u'he',\n",
       " u'him',\n",
       " u'his',\n",
       " u'himself',\n",
       " u'she',\n",
       " u'her',\n",
       " u'hers',\n",
       " u'herself',\n",
       " u'it',\n",
       " u'its',\n",
       " u'itself',\n",
       " u'they',\n",
       " u'them',\n",
       " u'their',\n",
       " u'theirs',\n",
       " u'themselves',\n",
       " u'what',\n",
       " u'which',\n",
       " u'who',\n",
       " u'whom',\n",
       " u'this',\n",
       " u'that',\n",
       " u'these',\n",
       " u'those',\n",
       " u'am',\n",
       " u'is',\n",
       " u'are',\n",
       " u'was',\n",
       " u'were',\n",
       " u'be',\n",
       " u'been',\n",
       " u'being',\n",
       " u'have',\n",
       " u'has',\n",
       " u'had',\n",
       " u'having',\n",
       " u'do',\n",
       " u'does',\n",
       " u'did',\n",
       " u'doing',\n",
       " u'a',\n",
       " u'an',\n",
       " u'the',\n",
       " u'and',\n",
       " u'but',\n",
       " u'if',\n",
       " u'or',\n",
       " u'because',\n",
       " u'as',\n",
       " u'until',\n",
       " u'while',\n",
       " u'of',\n",
       " u'at',\n",
       " u'by',\n",
       " u'for',\n",
       " u'with',\n",
       " u'about',\n",
       " u'against',\n",
       " u'between',\n",
       " u'into',\n",
       " u'through',\n",
       " u'during',\n",
       " u'before',\n",
       " u'after',\n",
       " u'above',\n",
       " u'below',\n",
       " u'to',\n",
       " u'from',\n",
       " u'up',\n",
       " u'down',\n",
       " u'in',\n",
       " u'out',\n",
       " u'on',\n",
       " u'off',\n",
       " u'over',\n",
       " u'under',\n",
       " u'again',\n",
       " u'further',\n",
       " u'then',\n",
       " u'once',\n",
       " u'here',\n",
       " u'there',\n",
       " u'when',\n",
       " u'where',\n",
       " u'why',\n",
       " u'how',\n",
       " u'all',\n",
       " u'any',\n",
       " u'both',\n",
       " u'each',\n",
       " u'few',\n",
       " u'more',\n",
       " u'most',\n",
       " u'other',\n",
       " u'some',\n",
       " u'such',\n",
       " u'no',\n",
       " u'nor',\n",
       " u'not',\n",
       " u'only',\n",
       " u'own',\n",
       " u'same',\n",
       " u'so',\n",
       " u'than',\n",
       " u'too',\n",
       " u'very',\n",
       " u's',\n",
       " u't',\n",
       " u'can',\n",
       " u'will',\n",
       " u'just',\n",
       " u'don',\n",
       " u'should',\n",
       " u'now',\n",
       " u'd',\n",
       " u'll',\n",
       " u'm',\n",
       " u'o',\n",
       " u're',\n",
       " u've',\n",
       " u'y',\n",
       " u'ain',\n",
       " u'aren',\n",
       " u'couldn',\n",
       " u'didn',\n",
       " u'doesn',\n",
       " u'hadn',\n",
       " u'hasn',\n",
       " u'haven',\n",
       " u'isn',\n",
       " u'ma',\n",
       " u'mightn',\n",
       " u'mustn',\n",
       " u'needn',\n",
       " u'shan',\n",
       " u'shouldn',\n",
       " u'wasn',\n",
       " u'weren',\n",
       " u'won',\n",
       " u'wouldn']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final_list_of_words=[]\n",
    "from nltk.corpus import stopwords\n",
    "english_stops = list(stopwords.words('english'))\n",
    "english_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_list_of_words=[each for each in RegEx_Tokenized_Words if each not in english_stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 was the number of stop words in a total word count of  139\n"
     ]
    }
   ],
   "source": [
    "print len(RegEx_Tokenized_Words)-len(Final_list_of_words), \"was the number of stop words in a total word count of \", len(RegEx_Tokenized_Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Final_list_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Stemming__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ghanian\n",
      "believ\n",
      "\n",
      "\n",
      "ghanian\n",
      "believ\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "print PorterStemmer().stem('ghanian')\n",
    "print PorterStemmer().stem('believing')\n",
    "\n",
    "print \"\\n\"\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "print stemmer.stem('ghanian')\n",
    "print stemmer.stem('believing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Final_list_of_words_stemmed=[]\n",
    "for each in Final_list_of_words:\n",
    "    Final_list_of_words_stemmed.append(stemmer.stem(each))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n",
      "[u'princip', u'compon', u'analysi', u'pca', u'statist', u'procedur', u'use', u'orthogon', u'transform', u'convert']\n",
      "[u'uncorrel', u'orthogon', u'basi', u'set', u'pca', u'sensit', u'relat', u'scale', u'origin', u'variabl']\n"
     ]
    }
   ],
   "source": [
    "print len(Final_list_of_words_stemmed)\n",
    "print Final_list_of_words_stemmed[:10]\n",
    "print Final_list_of_words_stemmed[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Lemmatizing__\n",
    "##Very similar to Stemming, but subtly different\n",
    "##lemmatizing is legible/understandable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy\n",
      "greet\n",
      "noun\n",
      "book\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizing=WordNetLemmatizer()\n",
    "print lemmatizing.lemmatize('happiest', pos='a')\n",
    "print lemmatizing.lemmatize('greets', pos='v')\n",
    "print lemmatizing.lemmatize('nouns', pos='n')\n",
    "print lemmatizing.lemmatize('booking', pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_list_of_words_lemmatize=[]\n",
    "for each in Final_list_of_words:\n",
    "    Final_list_of_words_lemmatize.append(lemmatizing.lemmatize(each,pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n",
      "['Principal', 'component', 'analysis', 'PCA', 'statistical', 'procedure', u'use', 'orthogonal', 'transformation', 'convert']\n",
      "['uncorrelated', 'orthogonal', 'basis', 'set', 'PCA', 'sensitive', 'relative', u'scale', 'original', 'variables']\n"
     ]
    }
   ],
   "source": [
    "print len(Final_list_of_words_lemmatize)\n",
    "print Final_list_of_words_lemmatize[:10]\n",
    "print Final_list_of_words_lemmatize[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Replacing words matching RegExp__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "replacement_patterns = [\n",
    "(r'won\\'t', 'will not'),\n",
    "(r'can\\'t', 'cannot'),\n",
    "(r'\\Wcant\\W',r' cannot '),\n",
    "(r'i\\s?\\'m', 'i am'),\n",
    "(r'ain\\'t', 'is not'),\n",
    "(r'([\\w\\s]+)\\'ll', '\\g<1> will'),\n",
    "(r'([\\w\\s]+)n\\'t', '\\g<1> not'),\n",
    "(r'([\\w\\s]+)\\'ve', '\\g<1> have'),\n",
    "(r'([\\w\\s]+)\\'s', '\\g<1> is'),\n",
    "(r'([\\w\\s]+)\\'re', '\\g<1> are'),\n",
    "(r'([\\w\\s]+)\\'d', '\\g<1> would')\n",
    "]\n",
    "class RegexpReplacer(object):\n",
    "    def __init__(self, patterns=replacement_patterns):\n",
    "        self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n",
    "\n",
    "    def replace(self, text):\n",
    "        s = text\n",
    "        for (pattern, repl) in self.patterns:\n",
    "            s = re.sub(pattern, repl, s,re.I)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I would  have been a good boy if it is not for that woman\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "replacer = RegexpReplacer()\n",
    "Ans= replacer.replace(\"I'd 've been a good boy if it ain't for that woman\")\n",
    "print Ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Replacing Repeating Characters__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good good good'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r'\\b(\\w+)( \\1\\b)+','This is good good good work',re.I).group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is good work'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'\\b(\\w+)( \\1\\b)+',r'\\1','This is good good good work',re.I)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
