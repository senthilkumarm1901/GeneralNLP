{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import re\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "import pandas as pd\n",
    "import hdbscan\n",
    "import sklearn\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.preprocessing import normalize\n",
    "import io\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links: \n",
    "- https://hdbscan.readthedocs.io/en/latest/ <br>\n",
    "- https://towardsdatascience.com/lightning-talk-clustering-with-hdbscan-d47b83d1b03a <br>\n",
    "- https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "def preprocess(str):\n",
    "    # remove links\n",
    "    str = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', \"\", str)\n",
    "    str = re.sub(r\"\\'s\", \" \\'s\", str)\n",
    "    str = re.sub(r\"\\'ve\", \" \\'ve\", str)\n",
    "    str = re.sub(r\"n\\'t\", \" n\\'t\", str)\n",
    "    str = re.sub(r\"\\'re\", \" \\'re\", str)\n",
    "    str = re.sub(r\"\\'d\", \" \\'d\", str)\n",
    "    str = re.sub(r\"\\'ll\", \" \\'ll\", str)\n",
    "    str = re.sub(r\",\", \" , \", str)\n",
    "    str = re.sub(r\"!\", \" ! \", str)\n",
    "    str = re.sub(r\"\\(\", \" ( \", str)\n",
    "    str = re.sub(r\"\\)\", \" ) \", str)\n",
    "    str = re.sub(r\"\\?\", \" ? \", str)\n",
    "    str = re.sub(r\"\\s{2,}\", \" \", str)\n",
    "    str = re.sub(\"(\\r)+\", \"\", str)\n",
    "    str = re.sub(\"(\\n)+\", \"\", str)\n",
    "    str = re.sub(r\"^\\s\",\"\",str)\n",
    "    return str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Documents(object):\n",
    "    def __init__(self, documents):\n",
    "        self.documents = documents\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i, doc in enumerate(self.documents):\n",
    "            yield TaggedDocument(words = doc, tags = [i])\n",
    "file = r\"some_file.csv\"\n",
    "corpus = open(file, \"r\", encoding=\"utf-8\")\n",
    "lines = corpus.read().split(\"\\n\")\n",
    "count = len(lines)\n",
    "preprocessed = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_dict = {}\n",
    "for t in lines:\n",
    "    #if t not in duplicate_dict:\n",
    "        #duplicate_dict[t] = True\n",
    "    t = preprocess(t)\n",
    "    fixed =''.join([x if x.isalnum() or x.isspace() else \" \" for x in t ]).split()\n",
    "    preprocessed.append(fixed)\n",
    "\n",
    "documents = Documents(preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(size=200, dbow_words=1, dm=0, window=12, seed=17, min_count=1, workers=4, iter=1000)\n",
    "\n",
    "model.build_vocab(documents)\n",
    "\n",
    "model.train(documents, total_examples=model.corpus_count, epochs=model.iter)\n",
    "model.save(r'pvdm.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = r\"pvdm.model\"\n",
    "model = Doc2Vec.load(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.docvecs.doctag_syn0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector_array = np.asarray(vectors)\n",
    "norm_data = normalize(model.docvecs.doctag_syn0, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#norm_data\n",
    "\n",
    "start_time=time.time()\n",
    "clusterer1 = hdbscan.HDBSCAN(min_cluster_size=8,min_samples=2, cluster_selection_method='leaf', core_dist_n_jobs=4)\n",
    "db = clusterer1.fit(norm_data)\n",
    "\n",
    "np.savetxt(r\"NEW_labels_1.csv\", clusterer1.labels_, delimiter=\",\")\n",
    "#np.savetxt(r\"NEW_labels_1.csv\", clusterer1.probabilities_, delimiter=\",\")\n",
    "\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = list(clusterer1.labels_)\n",
    "probab = list(clusterer1.probabilities_)\n",
    "df1['ClusterID'] = pd.Series(label, index=df1.index)\n",
    "df1['Probab'] = pd.Series(probab, index=df1.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(r\"Clustered_File.csv\", encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = df1['ClusterID'].value_counts()\n",
    "freq = freq.to_frame()\n",
    "freq.columns = ['Freq']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq.to_csv(r\"Frequency_File.csv\", encoding='utf-8', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = pd.DataFrame(norm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.concat([df1,norm], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df4.drop(df4.index[len(df4)-1])\n",
    "df4 = df4.groupby('ClusterID').first().reset_index()\n",
    "df4 = df4.drop('Probab',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.to_csv(r\"Level2.csv\",encoding='utf-8', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = np.asarray(df4.ix[1:,2:203])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start_time=time.time()\n",
    "clusterer2 = hdbscan.HDBSCAN(min_cluster_size=2,min_samples=None, cluster_selection_method='eom', core_dist_n_jobs=4)\n",
    "db2 = clusterer2.fit(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df4.ix[1:,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_2 = list(clusterer2.labels_)\n",
    "probab_2 = list(clusterer2.probabilities_)\n",
    "df_new['ClusterID_2'] = label_2\n",
    "df_new['Probab'] = probab_2\n",
    "df_new.to_csv(r\"Level2_Clustered_File.csv\", encoding=\"ISO-8859-1\")\n",
    "freq2 = df_new['ClusterID_2'].value_counts()\n",
    "freq2 = freq2.to_frame()\n",
    "freq2.columns = ['Freq']\n",
    "freq2.to_csv(r\"Level2_Frequency_File.csv\", encoding='utf-8', sep=\",\")\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_cluster in range(2, 50):\n",
    "    kmeans = KMeans(n_clusters=n_cluster).fit(df5)\n",
    "    label = kmeans.labels_\n",
    "    sil_coeff = silhouette_score(df5, label, metric='euclidean')\n",
    "    print(\"For n_clusters={}, The Silhouette Coefficient is {}\".format(n_cluster, sil_coeff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=10, max_iter=1000).fit(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_\n",
    "np.savetxt(r\"Kmeans.csv\", kmeans.labels_, delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
